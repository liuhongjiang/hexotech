<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>SMO序列最小最优化算法 | Andrew&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="上一篇blog讲到了svm的原理，最后将需要解决问题抽象成了数学公式，但如何利用计算机，解出这些数学公式的答案。换句话说，就是怎么通过计算机算出我们的svm模型的参数呢？方法就是序列最小最优化(sequential minimal optimization, SMO)算法。
首先回顾一下SVM模型的数学表达，即svm的对偶问题:
\[
\begin{array}{l}
\mathop {\">
<meta property="og:type" content="article">
<meta property="og:title" content="SMO序列最小最优化算法">
<meta property="og:url" content="http://liuhongjiang.github.io/2012/12/28/svm-smo/index.html">
<meta property="og:site_name" content="Andrew's Blog">
<meta property="og:description" content="上一篇blog讲到了svm的原理，最后将需要解决问题抽象成了数学公式，但如何利用计算机，解出这些数学公式的答案。换句话说，就是怎么通过计算机算出我们的svm模型的参数呢？方法就是序列最小最优化(sequential minimal optimization, SMO)算法。
首先回顾一下SVM模型的数学表达，即svm的对偶问题:
\[
\begin{array}{l}
\mathop {\">
<meta property="og:image" content="http://liuhongjiang.github.io/hexotech/images/blogimages/2012/svm/svm_sep_ok.png">
<meta property="og:image" content="http://liuhongjiang.github.io/hexotech/images/blogimages/2012/svm/svm_not_sep.png">
<meta property="og:updated_time" content="2015-07-23T05:46:28.450Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SMO序列最小最优化算法">
<meta name="twitter:description" content="上一篇blog讲到了svm的原理，最后将需要解决问题抽象成了数学公式，但如何利用计算机，解出这些数学公式的答案。换句话说，就是怎么通过计算机算出我们的svm模型的参数呢？方法就是序列最小最优化(sequential minimal optimization, SMO)算法。
首先回顾一下SVM模型的数学表达，即svm的对偶问题:
\[
\begin{array}{l}
\mathop {\">
  
    <link rel="alternative" href="/atom.xml" title="Andrew&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/hexotech/images/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/hexotech/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/hexotech/" id="logo">Andrew&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/hexotech/">Home</a>
        
          <a class="main-nav-link" href="/hexotech/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://liuhongjiang.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-svm-smo" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/hexotech/2012/12/28/svm-smo/" class="article-date">
  <time datetime="2012-12-28T01:01:00.000Z" itemprop="datePublished">2012-12-28</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/hexotech/categories/Machine-nbsp-Learning/">Machine&nbsp;Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      SMO序列最小最优化算法
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>上一篇blog讲到了<a href="/blog/2012/12/26/svm/">svm的原理</a>，最后将需要解决问题抽象成了数学公式，但如何利用计算机，解出这些数学公式的答案。换句话说，就是怎么通过计算机算出我们的svm模型的参数呢？方法就是序列最小最优化(sequential minimal optimization, SMO)算法。</p>
<p>首先回顾一下SVM模型的数学表达，即svm的对偶问题:</p>
<p><span class="math display">\[
\begin{array}{l}
\mathop {\min }\limits_a \qquad \frac{1}{2}\sum\limits_{i = 1}^N {\sum\limits_{j = 1}^N { {a_i}{a_j}{y_i}{y_j}K({x_i},{x_j}) - \sum\limits_{i = 1}^N { {a_i}} } } \\
{\rm{s.t.}}\qquad\sum\limits_{i = 1}^N { {a_i}{y_i} = 0} \\
\qquad\qquad 0 \le {a_i} \le C, \qquad i = 1,2,\cdot\cdot\cdot,N
\end{array}
\]</span></p>
<p>选择一个 $ {a^*} $ 的正分量 $ 0  C $ , 计算（或者通过所有解求平均值）：</p>
<p><span class="math display">\[
{b^*} = {y_j} - \sum\limits_{i = 1}^N {a_i^*{y_i}K({x_i} \cdot {x_j})} 
\]</span></p>
<p>决策函数为</p>
<p><span class="math display">\[
f(x) = sign(\sum\limits_{i=1}^N {a_i^*{y_i}K({x_i}, {x_j})} + {b^*})
\]</span></p>
<p>svm的学习，就是通过训练数据计算出<span class="math inline">\({a^\*}\)</span>和<span class="math inline">\({b^\*}\)</span>，然后通过决策函数判定<span class="math inline">\({x_j}\)</span>的分类。其中<span class="math inline">\({a^\*}\)</span>是一个向量，长度与训练数据的样本数相同，如果训练数据很大，那么这个向量会很长，不过绝大部分的分量值都是0，只有支持向量的对应的分量值大于0 。</p>
<p>SMO是一种启发式算法，其基本思想是：如果所有变量的解都满足了此最优化问题的KKT条件，那么这个最优化问题的解就得到了。否则，选择两个变量，固定其它变量，针对这两个变量构建一个二次规划问题，然后关于这个二次规划的问题的解就更接近原始的二次归还问题的解，因为这个解使得需要优化的问题的函数值更小。</p>
<p>翻译一下：对于svm我们要求解<span class="math inline">\({a^\*}\)</span>，如果 <span class="math inline">\({a^\*}\)</span> 的所有分量满足svm对偶问题的KKT条件，那么这个问题的解就求出来了，我们svm模型学习也就完成了。如果没有满足KKT，那么我们就在 <span class="math inline">\({a^\*}\)</span> 中找两个分量 <span class="math inline">\({a_i}\)</span> 和 <span class="math inline">\({a_j}\)</span>，其中 <span class="math inline">\({a_i}\)</span> 是违反KKT条件最严重的分量，通过计算，使得 <span class="math inline">\({a_i}\)</span> 和 <span class="math inline">\({a_j}\)</span> 满足KKT条件，直到<span class="math inline">\({a^\*}\)</span> 的所有分量都满足KKT条件。而且这个计算过程是收敛的，因为每次计算出来的新的两个分量，使得对偶问题中要优化的目标函数（就是min对应的那个函数）值更小。至于为什么是收敛的，是因为，每次求解的那两个分量，是要优化问题在这两个分量上的极小值，所以每一次优化，都会使目标函数比上一次的优化结果的值变小。</p>
<a id="more"></a>
<p>我们来看看KKT条件。</p>
<h2 id="kkt">KKT</h2>
<p>上面的问题，是通过svm的原始问题，构造拉格朗日函数，并通过对偶换算得出的对偶问题。与对偶问题等价的是对偶问题的KKT条件，参考<a href="http://book.douban.com/subject/10590856/" target="_blank" rel="external">《统计学习方法》</a>的附录C的定理C.3。换句话说，就是只要找到对应的<span class="math inline">\({a^\*}\)</span>满足了下列KKT条件，那么原始问题和对偶问题就解决了。</p>
<p>SVM的对偶问题对应的KKT条件为：</p>
<p><span class="math display">\[
\begin{array}{l}
\quad {a_i} = 0 \quad \Leftrightarrow \quad {y_i}g({x_i}) \ge 1\\
0 &lt; {a_i} &lt; C \quad \Leftrightarrow \quad {y_i}g({x_i}) = 1\\
\quad {a_i} = C \quad \Leftrightarrow \quad {y_i}g({x_i}) \le 1
\end{array}
\]</span></p>
<p>其中:</p>
<p><span class="math display">\[
g(x) = \sum\limits_{i = 1}^N { {a_i}{y_i}K({x_i},{x_j}) + b} 
\]</span></p>
<p>因为计算机在计算的时候是有精度范围的，所以我们引入一个计算精度值<span class="math inline">\(\varepsilon\)</span>，</p>
<p><span class="math display">\[
\left\{ \begin{array}{l}
{a_i} = 0 \Leftrightarrow {y_i}g({x_i}) \ge 1 - \varepsilon \\
0 &lt; {a_i} &lt; C \Leftrightarrow 1 - \varepsilon  \le {y_i}g({x_i}) \le 1 + \varepsilon \\
{a_i} = C \Leftrightarrow {y_i}g({x_i}) \le 1 + \varepsilon 
\end{array} \right\} \Rightarrow \left\{ \begin{array}{l}
{a_i} &lt; C \Leftrightarrow 1 - \varepsilon  \le {y_i}g({x_i})\\
0 &lt; {a_i} \Leftrightarrow {y_i}g({x_i}) \le 1 + \varepsilon 
\end{array} \right\}
\]</span></p>
<p>同时由于<span class="math inline">\({y_i} = \pm 1\)</span>，所以<span class="math inline">\({y_i}\*{y_i}=1\)</span>，上面的公式可以换算为</p>
<p><span class="math display">\[
\begin{array}{l}
{a_i} &lt; C \Leftrightarrow  - \varepsilon  \le {y_i}(g({x_i}) - {y_i})\\
        0 &lt; {a_i} \Leftrightarrow {y_i}(g({x_i}) - {y_i}) \le  + \varepsilon 
        \end{array}
\]</span></p>
<p>定义:</p>
<p><span class="math display">\[
{E_i} = g({x_i}) - {y_i}
\]</span></p>
<p>其中，<span class="math inline">\(g({x})\)</span>其实就是决策函数，所以<span class="math inline">\({E_i}\)</span>可以认为是对输入的<span class="math inline">\({x_i}\)</span>的预测值与真实输出<span class="math inline">\({y_i}\)</span>之差。</p>
<p>上面的公式就可以换算为，即KKT条件可以表示为：</p>
<p><span class="math display">\[
\begin{array}{l}
{a_i} &lt; C \Leftrightarrow  - \varepsilon  \le {y_i}{E_i}\\
        0 &lt; {a_i} \Leftrightarrow {y_i}{E_i} \le  + \varepsilon 
        \end{array}
\]</span></p>
<p>那么相应的违规KKT条件的分量应该满足下列不等式：</p>
<p><span class="math display">\[
{\rm{Against\ KKT:}}
\]</span></p>
<p><span class="math display">\[
\begin{array}{l}
{a_i} &lt; C \quad \Leftrightarrow \quad  - \varepsilon  &gt; {y_i}{E_i}\\
0 &lt; {a_i} \quad \Leftrightarrow \quad {y_i}{E_i} &gt;  + \varepsilon 
\end{array}
\]</span></p>
<p>其实上面的推导过程不必关心，只需要应用违犯KKT条件的公式就可以了。</p>
<h2 id="smo算法描述">SMO算法描述</h2>
<p style="text-indent:0">
<em style="color:blue"> 输入：</em>训练数据集 $ T=\{({x_1},{y_1}),({x_2},{y_2}), ,({x_N},{y_N})\} $
</p>
<p>其中$ {x_i} <span class="math inline">\(，\)</span>{y_i} \{-1,+1\}<span class="math inline">\(，\)</span>i=1,2,,N<span class="math inline">\(，精度\)</span>$。</p>
<p style="text-indent:0">
<em style="color:blue">输出：</em>近似解<span class="math inline">\(\hat a\)</span>
</p>
<p style="text-indent:0">
<em style="color:blue">算法描述：</em>
</p>
<ol style="list-style-type: decimal">
<li><p>取初始值<span class="math inline">\({a^{(0)}}=0\)</span>，令<span class="math inline">\(K=0\)</span></p></li>
<li><p>选取优化变量 <span class="math inline">\({a\_1^{(k)}}\)</span> , <span class="math inline">\({a\_2^{(k)}}\)</span> , 针对优化问题，求得最优解 <span class="math inline">\({a\_1^{(k+1)}}\)</span> , <span class="math inline">\({a\_2^{(k+1)}}\)</span> 更新 <span class="math inline">\({a^{(k)}}\)</span> 为 <span class="math inline">\({a^{(k+1)}}\)</span> 。</p></li>
<li><p>在精度条件范围内是否满足停机条件，即是否有变量违反KKT条件，如果违反了，则令<span class="math inline">\(k=k+1\)</span>，跳转(2)，否则(4)。</p></li>
<li><p>求得近似解<span class="math inline">\(\hat a = a^{(k+1)}\)</span></p></li>
</ol>
<p>上面算法的(1)、(3)、(4)步都不难理解，其中第(3)步中，是否违反KKT条件，对于<span class="math inline">\(a^{(k)}\)</span>的每个分量按照上一节的违反KKT条件的公式进行验算即可。难于理解的是第(2)步，下面就重点解释优化变量选取和如何更新选取变量。</p>
<h3 id="变量选取">变量选取</h3>
<p>变量选取分为两步，第一步是选取违反KKT条件最严重的<span class="math inline">\({a\_i}\)</span>，第二步是根据已经选取的第一个变量，选择优化程度最大的第二个变量。</p>
<p>违反KKT条件最严重的变量可以按照这样的规则选取，首先看<span class="math inline">\(0 \lt {a\_i} \lt C\)</span>的那些分量中，是否有违反KKT条件的，如果有，则选取<span class="math inline">\({y\_i}g({x\_i})\)</span>最小的那个做为<span class="math inline">\({a\_1}\)</span>。如果没有则遍历所有的样本点，在违反KKT条件的分量中选取<span class="math inline">\({y\_i}g({x\_i})\)</span>最小的做为<span class="math inline">\({a\_1}\)</span>。</p>
<p>当选择了<span class="math inline">\({a\_1}\)</span>后，如果<span class="math inline">\({a\_1}\)</span>对应的<span class="math inline">\(E\_1\)</span>为正，选择<span class="math inline">\(E\_i\)</span>最小的那个分量最为<span class="math inline">\({a\_2}\)</span>，如果<span class="math inline">\(E\_1\)</span>为负，选择<span class="math inline">\(E\_i\)</span>最大的那个分量最为<span class="math inline">\({a\_2}\)</span>，这是因为<span class="math inline">\({a\_2^{new}}\)</span>依赖于<span class="math inline">\(\left| {E\_1 - E\_2} \right|\)</span>（后面的公式会讲到）。 如果选择的<span class="math inline">\({a\_2}\)</span>，不能满足下降的最小步长，那么就遍历所有的支持向量点做为<span class="math inline">\({a\_2}\)</span>进行试用，如果仍然都不能满足下降的最小步长，那么就遍历所有的样本点做为<span class="math inline">\({a\_2}\)</span>试用。如果还算是不能满足下降的最小步长，那么就重新选择<span class="math inline">\({a\_1}\)</span>。</p>
<h3 id="计算选取变量的新值">计算选取变量的新值</h3>
<p>首先计算出来的新值必须满足约束条件<span class="math inline">\(\sum\limits\_{i = 1}^N { {a\_i}{y\_i} = 0}\)</span> ，那么求出来的<span class="math inline">\({a\_2^{new}}\)</span>需要满足下列条件（具体推导见《统计学习方法》的7.4.1）:</p>
<p><span class="math display">\[
\begin{array}{l}
L \le a_2^{new} \le H\\
L = \max (0,a_2^{old} - a_1^{old}),H = \min (C,C + a_2^{old} - a_1^{old}), \qquad {y_1} \ne {y_2}\\
L = \max (0,a_2^{old} + a_1^{old} - C),H = \min (C,a_2^{old} + a_1^{old}), \qquad {y_1} = {y_2}
\end{array}
\]</span></p>
<p>未经过裁剪的<span class="math inline">\({a\_2}\)</span>的解为：</p>
<p><span class="math display">\[
\begin{array}{l}
{a_2^{new,unc}} = {a_2^{old}} + \frac{ {y_2}({E_1}-{E_2)}}{\eta} \\
\eta = K_{11} + K_{22} - 2{K_{12}}
\end{array}
\]</span></p>
<p>裁剪后的解为</p>
<p><span class="math display">\[
a_2^{new} = \left\{ \begin{array}{l}
H,a_2^{new,unc} &gt; H\\
        a_2^{new,unc},L \le a_2^{new,unc} \le H\\
        L,a_2^{new,unc} &lt; L
        \end{array} \right.
\]</span></p>
<p>第一个变量的解为</p>
<p><span class="math display">\[
a_1^{new} = a_1^{old} + {y_1}{y_2}(a_2^{old} - a_2^{new})
\]</span></p>
<p>还需要更新<span class="math inline">\(b\)</span>:</p>
<p><span class="math display">\[
\begin{array}{l}
b_1^{new} =  - {E_1} - {y_1}{K_{11}}(a_1^{new} - a_1^{old}) - {y_2}{K_{21}}(a_2^{new} - a_2^{old}) + {b^{old}}\\
        b_2^{new} =  - {E_2} - {y_1}{K_{12}}(a_1^{new} - a_1^{old}) - {y_2}{K_{22}}(a_2^{new} - a_2^{old}) + {b^{old}}
        \end{array}
\]</span></p>
<p>在更新<span class="math inline">\(b\)</span>时，如果有<span class="math inline">\(0 \lt a\_1^{new} \lt C\)</span>, 则<span class="math inline">\(b^{new}=b\_1^{new}\)</span>，如果有<span class="math inline">\(0 \lt a\_2^{new} \lt C\)</span>, 则 <span class="math inline">\(b^{new}=b\_2^{new}\)</span>， 否则<span class="math inline">\(b^{new}=\frac{b\_1^{new} + b\_2^{new}}{2}\)</span>。</p>
<p>由于缓存了<span class="math inline">\({E\_i}\)</span>,所以需要计算新的<span class="math inline">\({E\_i}\)</span>:</p>
<p><span class="math display">\[
E_i^{new} = \sum\limits_{j=1}^N { {y_j}{a_j}K({x_i},{x_j})} + b^{new} - y_i
\]</span></p>
<h2 id="smo的一个实现例子">SMO的一个实现例子</h2>
<p>我实现了一个简单的基于SMO的线性svm，是一个python脚本。实现的过程中，变量的选取并未严格按照算法讲的方法选取，选择了一个简单的选取方法。 一次迭代中，遍历所有的<span class="math inline">\({a\_i}\)</span>，如果<span class="math inline">\({a\_i}\)</span>违反了KKT条件，那么就将它做为第一个变量，然后再遍历所有的<span class="math inline">\({a\_i}\)</span>，依次做为第二个变量，如果第二个变量有足够的下降，那么就更新两个变量。如果没有，就不更新。</p>
<p>实现的python脚本如下：</p>
<figure class="highlight python"><figcaption><span>使用python实现的基于SMO的SVM</span><a href="/hexotech/code/2012/smo/smo.py">view raw</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># SMO的一个简单实现</span></span><br><span class="line"><span class="comment"># implement SMO</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">samples = []</span><br><span class="line">labels = []</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">svm_params</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.a = []</span><br><span class="line">        self.b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">params = svm_params()</span><br><span class="line">e_dict = []</span><br><span class="line"></span><br><span class="line"><span class="comment">#train_data = "svm.train_mix_ok"</span></span><br><span class="line">train_data = <span class="string">"svm.train"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loaddata</span><span class="params">()</span>:</span></span><br><span class="line">    fn = open(train_data,<span class="string">"r"</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fn:</span><br><span class="line">        line = line[:-<span class="number">1</span>]</span><br><span class="line">        vlist = line.split(<span class="string">"\t"</span>)</span><br><span class="line">        samples.append((int(vlist[<span class="number">0</span>]), int(vlist[<span class="number">1</span>])))</span><br><span class="line">        labels.append(int(vlist[<span class="number">2</span>]))</span><br><span class="line">        params.a.append(<span class="number">0.0</span>)</span><br><span class="line">    fn.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># linear</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel</span><span class="params">(j, i)</span>:</span></span><br><span class="line">    ret = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(len(samples[j])):</span><br><span class="line">        ret += samples[j][idx] * samples[i][idx]</span><br><span class="line">    <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_real_diff</span><span class="params">(i)</span>:</span></span><br><span class="line">    diff = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(samples)):</span><br><span class="line">        diff += params.a[j] * labels[j] * kernel(j,i)</span><br><span class="line">    diff = diff + params.b - labels[i]</span><br><span class="line">    <span class="keyword">return</span> diff</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_e_dict</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(params.a)):</span><br><span class="line">        e_dict.append(predict_real_diff(i))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_e_dict</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(params.a)):</span><br><span class="line">        e_dict[i] = predict_real_diff(i)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(tolerance, times, C)</span>:</span></span><br><span class="line">    time = <span class="number">0</span></span><br><span class="line">    init_e_dict()</span><br><span class="line">    updated = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">while</span> time &lt; times <span class="keyword">and</span> updated:</span><br><span class="line">        updated = <span class="keyword">False</span></span><br><span class="line">        time += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(params.a)):</span><br><span class="line">            ai = params.a[i]</span><br><span class="line">            Ei = e_dict[i]</span><br><span class="line">            <span class="comment"># 违反KKT</span></span><br><span class="line">            <span class="comment"># agaist the KKT</span></span><br><span class="line">            <span class="keyword">if</span> (labels[i] * Ei &lt; -tolerance <span class="keyword">and</span> ai &lt; C) <span class="keyword">or</span> (labels[i] * Ei &gt; tolerance <span class="keyword">and</span> ai &gt; <span class="number">0</span>):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(len(params.a)):</span><br><span class="line">                    <span class="keyword">if</span> j == i: <span class="keyword">continue</span></span><br><span class="line">                    eta = kernel(i, i) + kernel(j, j) - <span class="number">2</span> * kernel(i, j)</span><br><span class="line">                    <span class="keyword">if</span> eta &lt;= <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    new_aj = params.a[j] + labels[j] * (e_dict[i] - e_dict[j]) / eta </span><br><span class="line">                    L = <span class="number">0.0</span></span><br><span class="line">                    H = <span class="number">0.0</span></span><br><span class="line">                    <span class="keyword">if</span> labels[i] == labels[j]:</span><br><span class="line">                        L = max(<span class="number">0</span>, params.a[j] + params.a[i] - C)</span><br><span class="line">                        H = min(C, params.a[j] + params.a[i])</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        L = max(<span class="number">0</span>, params.a[j] - params.a[i]) </span><br><span class="line">                        H = min(C, C + params.a[j] - params.a[i])</span><br><span class="line">                    <span class="keyword">if</span> new_aj &gt; H:</span><br><span class="line">                        new_aj = H</span><br><span class="line">                    <span class="keyword">if</span> new_aj &lt; L:</span><br><span class="line">                        new_aj = L</span><br><span class="line">                    <span class="comment"># 《统计学习方法》公式7.109（下同）</span></span><br><span class="line">                    <span class="comment"># formula 7.109</span></span><br><span class="line">                    new_ai = params.a[i] + labels[i] * labels[j] * (params.a[j] - new_aj)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 第二个变量下降是否达到最小步长</span></span><br><span class="line">                    <span class="comment"># decline enough for new_aj</span></span><br><span class="line">                    <span class="keyword">if</span> abs(params.a[j] - new_aj) &lt; <span class="number">0.001</span>:</span><br><span class="line">                        <span class="keyword">print</span> <span class="string">"j = %d, is not moving enough"</span> % j</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># formula 7.115</span></span><br><span class="line">                    new_b1 = params.b - e_dict[i] - labels[i]*kernel(i,i)*(new_ai-params.a[i]) - labels[j]*kernel(j,i)*(new_aj-params.a[j]) </span><br><span class="line">                    <span class="comment"># formula 7.116</span></span><br><span class="line">                    new_b2 = params.b - e_dict[j] - labels[i]*kernel(i,j)*(new_ai-params.a[i]) - labels[j]*kernel(j,j)*(new_aj-params.a[j]) </span><br><span class="line">                    <span class="keyword">if</span> new_ai &gt; <span class="number">0</span> <span class="keyword">and</span> new_ai &lt; C: new_b = new_b1</span><br><span class="line">                    <span class="keyword">elif</span> new_aj &gt; <span class="number">0</span> <span class="keyword">and</span> new_aj &lt; C: new_b = new_b2</span><br><span class="line">                    <span class="keyword">else</span>: new_b = (new_b1 + new_b2) / <span class="number">2.0</span></span><br><span class="line">                    </span><br><span class="line">                    params.a[i] = new_ai</span><br><span class="line">                    params.a[j] = new_aj</span><br><span class="line">                    params.b = new_b</span><br><span class="line">                    update_e_dict()</span><br><span class="line">                    updated = <span class="keyword">True</span></span><br><span class="line">                    <span class="keyword">print</span> <span class="string">"iterate: %d, changepair: i: %d, j:%d"</span> %(time, i, j)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(tolerance, C)</span>:</span></span><br><span class="line">    plt.xlabel(<span class="string">u"x1"</span>)</span><br><span class="line">    plt.xlim(<span class="number">0</span>, <span class="number">100</span>)</span><br><span class="line">    plt.ylabel(<span class="string">u"x2"</span>)</span><br><span class="line">    plt.ylim(<span class="number">0</span>, <span class="number">100</span>)</span><br><span class="line">    plt.title(<span class="string">"SVM - %s, tolerance %f, C %f"</span> % (train_data, tolerance, C))</span><br><span class="line">    ftrain = open(train_data, <span class="string">"r"</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> ftrain:</span><br><span class="line">        line = line[:-<span class="number">1</span>]</span><br><span class="line">        sam = line.split(<span class="string">"\t"</span>)</span><br><span class="line">        <span class="keyword">if</span> int(sam[<span class="number">2</span>]) &gt; <span class="number">0</span>:</span><br><span class="line">            plt.plot(sam[<span class="number">0</span>],sam[<span class="number">1</span>], <span class="string">'or'</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            plt.plot(sam[<span class="number">0</span>],sam[<span class="number">1</span>], <span class="string">'og'</span>)</span><br><span class="line">    </span><br><span class="line">    w1 = <span class="number">0.0</span> </span><br><span class="line">    w2 = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(labels)):</span><br><span class="line">        w1 += params.a[i] * labels[i] * samples[i][<span class="number">0</span>]</span><br><span class="line">        w2 += params.a[i] * labels[i] * samples[i][<span class="number">1</span>]</span><br><span class="line">    w = - w1 / w2</span><br><span class="line"></span><br><span class="line">    b = - params.b / w2</span><br><span class="line">    r = <span class="number">1</span> / w2</span><br><span class="line"></span><br><span class="line">    lp_x1 = [<span class="number">10</span>, <span class="number">90</span>]</span><br><span class="line">    lp_x2 = []</span><br><span class="line">    lp_x2up = []</span><br><span class="line">    lp_x2down = []</span><br><span class="line">    <span class="keyword">for</span> x1 <span class="keyword">in</span> lp_x1:</span><br><span class="line">        lp_x2.append(w * x1 + b)</span><br><span class="line">        lp_x2up.append(w * x1 + b + r)</span><br><span class="line">        lp_x2down.append(w * x1 + b - r)</span><br><span class="line">    plt.plot(lp_x1, lp_x2, <span class="string">'b'</span>)</span><br><span class="line">    plt.plot(lp_x1, lp_x2up, <span class="string">'b--'</span>)</span><br><span class="line">    plt.plot(lp_x1, lp_x2down, <span class="string">'b--'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    loaddata()</span><br><span class="line">    <span class="keyword">print</span> samples</span><br><span class="line">    <span class="keyword">print</span> labels</span><br><span class="line">    <span class="comment"># 惩罚系数</span></span><br><span class="line">    <span class="comment"># penalty for mis classify</span></span><br><span class="line">    C = <span class="number">10</span></span><br><span class="line">    <span class="comment"># 计算精度</span></span><br><span class="line">    <span class="comment"># computational accuracy </span></span><br><span class="line">    tolerance = <span class="number">0.0001</span></span><br><span class="line">    train(tolerance, <span class="number">100</span>, C)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"a = "</span>, params.a</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"b = "</span>, params.b</span><br><span class="line">    support =  []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(params.a)):</span><br><span class="line">        <span class="keyword">if</span> params.a[i] &gt; <span class="number">0</span> <span class="keyword">and</span> params.a[i] &lt; C:</span><br><span class="line">            support.append(samples[i])</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"support vector = "</span>, support</span><br><span class="line">    draw(tolerance, C)</span><br></pre></td></tr></table></figure>
<p>脚本使用的训练数据可以下载<a href="https://github.com/liuhongjiang/blog_projects/tree/master/svm" target="_blank" rel="external">SMO实现的代码</a>的svm.train文件，或者使用<a href="https://github.com/liuhongjiang/blog_projects/blob/master/svm/blog_linear.py" target="_blank" rel="external">blog_linear.py</a>，通过改变变量<code>separable</code>可以生成能够完全划分开的样本和不能划分开的样本。</p>
<p>这个smo.py脚本是一个线性的svm，替换掉脚本中<code>kernel</code>函数，就可以成为一个非线性的svm。 下面这两张图片是用训练数据训练的结果。这一张是样本能完全分离开的:</p>
<img src="/hexotech/images/blogimages/2012/svm/svm_sep_ok.png" class="center">
<p>这一张是样本不能完全分离开的：</p>
<img src="/hexotech/images/blogimages/2012/svm/svm_not_sep.png" class="center">
<p>以上就是如何实现SMO的全部内容。之前的一个同事实现了一个简单的识别手写数字ocr，下一章，我们也来用svm实现一个简单的识别数字的ocr吧。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://liuhongjiang.github.io/hexotech/2012/12/28/svm-smo/" data-id="cieuoamug001wt4i4tzo1pm2q" class="article-share-link">Share</a>
      
        <a href="http://liuhongjiang.github.io/hexotech/2012/12/28/svm-smo/#disqus_thread" class="article-comment-link">Comments</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/hexotech/2012/12/29/svm-ocr/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          基于SVM的手写数字识别
        
      </div>
    </a>
  
  
    <a href="/hexotech/2012/12/26/svm/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">SVM支持向量机</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/hexotech/categories/3rd-nbsp-tools/">3rd&nbsp;tools</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/hexotech/categories/Algorithm/">Algorithm</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/hexotech/categories/C-C/">C/C++</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/hexotech/categories/Data-nbsp-structure/">Data&nbsp;structure</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/hexotech/categories/Fuck-nbsp-GFW/">Fuck&nbsp;GFW</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/hexotech/categories/Language/">Language</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/hexotech/categories/Machine-nbsp-Learning/">Machine&nbsp;Learning</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/hexotech/categories/Vi/">Vi</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/hexotech/categories/git/">git</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/hexotech/categories/hadoop/">hadoop</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/hexotech/categories/javascript/">javascript</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/hexotech/categories/linux/">linux</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/hexotech/categories/math/">math</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/hexotech/categories/octopress/">octopress</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/hexotech/categories/paper/">paper</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/hexotech/categories/sqlite/">sqlite</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/hexotech/archives/2015/09/">September 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/hexotech/archives/2015/08/">August 2015</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/hexotech/archives/2015/06/">June 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/hexotech/archives/2015/05/">May 2015</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/hexotech/archives/2015/04/">April 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/hexotech/archives/2014/05/">May 2014</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/hexotech/archives/2013/11/">November 2013</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/hexotech/archives/2013/08/">August 2013</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/hexotech/archives/2013/02/">February 2013</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/hexotech/archives/2013/01/">January 2013</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/hexotech/archives/2012/12/">December 2012</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/hexotech/archives/2012/11/">November 2012</a><span class="archive-list-count">7</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/hexotech/2015/09/22/Docker-beginning/">初识Docker</a>
          </li>
        
          <li>
            <a href="/hexotech/2015/09/07/Authentication-Anomaly-Detection-A-Case-Study-On-A-Virtual-Private-Network/">Authentication Anomaly Detection: A Case Study On A Virtual Private Network</a>
          </li>
        
          <li>
            <a href="/hexotech/2015/08/31/jenkins-tips/">Jenkins tips</a>
          </li>
        
          <li>
            <a href="/hexotech/2015/08/26/install-kafka/">Install kafka</a>
          </li>
        
          <li>
            <a href="/hexotech/2015/08/26/install-zookeeper/">Install zookeeper</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 Andrew Liu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/hexotech/" class="mobile-nav-link">Home</a>
  
    <a href="/hexotech/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    
<script>
  var disqus_shortname = 'andrewliu117';
  
  var disqus_url = 'http://liuhongjiang.github.io/hexotech/2012/12/28/svm-smo/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="/hexotech/js/jquery-2.1.4.min.js" type="text/javascript"></script>

<!--<script type="text/x-mathjax-config">-->
<!--MathJax.Hub.Config({-->
  <!--tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}-->
<!--});-->
<!--</script>-->
<!--<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">-->
<!--</script>-->


  <link rel="stylesheet" href="/hexotech/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/hexotech/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/hexotech/js/script.js" type="text/javascript"></script>


<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>